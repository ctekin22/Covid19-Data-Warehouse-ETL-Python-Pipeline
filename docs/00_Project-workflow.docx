Project Workflow: AWS-Based Data Pipeline for COVID-19 Dataset

Step 1: Log in with the IAM user you created
•	Goal: Securely access AWS services using an IAM (Identity and Access Management) user with limited permissions.
•	What to Do:
o	Go to AWS Management Console.
o	Sign in using the IAM user credentials (not the root user).
o	Ensure the IAM user has permissions for:
	Amazon S3 
	Amazon Athena
	AWS Glue
	Amazon Redshift 
•	Why It Matters:
o	IAM users ensure principle of least privilege, keeping AWS access secure and auditable. You should avoid using the root user for operational tasks.
 

Step 2: Upload the dataset to your S3 bucket
•	Goal: Store the raw COVID-19 dataset in a centralized, durable, and scalable storage (S3) for processing.
•	What to Do:
o	Go to Amazon S3 service.
o	Navigate to the S3 bucket you created 
o	Upload your dataset files manually or 
o	Use the AWS Console or the AWS CLI:
aws s3 cp /path/to/local/file.csv s3://de-covid19-data-de/
•	Why It Matters:
o	Amazon S3 is the source layer in your modern data architecture.
o	All downstream services (Glue crawler, Athena, Redshift) will reference this data.

 

Step 3: Use AWS Glue Crawler to Analyze Data Structure
Goal:
Automatically discover the schema and metadata of your data to build an accurate data model before processing.

What to Do:
Create a Glue crawler for each dataset:
•	Name as table name for easy identification
•	Specify the crawler source type:
o	Since you are starting fresh, choose Data stores as the data source.
o	Select Amazon S3 as the data store type.
•	Set the connection to No connection or keep default if not using a VPC connection.
•	Important: Provide the path to the folder (S3 bucket + folder path), not individual files. The crawler will scan all files inside the folder automatically.
•	Run the crawler to extract schema details such as number of columns, data types, and partitions.
Create an IAM Role for Glue:
•	Go to the AWS IAM console and create a new role.
•	Select Glue as the trusted service.
•	Attach the following permissions policies:
o	AmazonS3FullAccess
o	AWSGlueServiceRole
o	AWSGlueConsoleFullAccess
•	You can create this role during the crawler setup or beforehand. This role allows Glue to access your S3 data and manage resources.
Create or Select Glue Database:
•	Create a new Glue database (covid_19) to organize your crawled tables.
Crawler Schedule:
•	You can configure the crawler to run on-demand or schedule it (hourly, daily, weekly). For initial runs, manual triggering is recommended.
Run the crawler. It usually takes 1-2 minutes to catalog the data and create table metadata in the Glue Data Catalog. Repeat this process for each dataset you want to catalog.
What Happens Internally:
•	When you create and run a crawler, it automatically creates a table in the Glue Data Catalog.
•	This catalog stores all metadata related to your data files, such as column names, types, record counts, and file locations.
•	The catalog acts as a centralized metadata repository, which Athena and other AWS services query to understand your data structure.
•	Using Glue and Athena together means you don’t have to manually manage schemas; Glue handles it for you.
Why It Matters:
•	The Glue crawler automatically catalogs your data, saving manual schema definition effort.
•	It ensures your ETL jobs and queries work with up-to-date and accurate metadata.

Step 4: Analyze Data with Amazon Athena and Build Data Model
Goal:
Use Amazon Athena to query and analyze your crawled data directly on S3 using standard SQL, enabling quick insights and data model building without loading data elsewhere.
What to Do:
•	Open Amazon Athena in the AWS Console.
•	Set the Data source to AwsDataCatalog and select the Glue database you created (e.g., covid_19).
•	Before running queries, configure a query result location in S3:
o	Create a dedicated S3 bucket for Athena query outputs, e.g., de-covid19-athena-output.
o	Set this bucket as the output location in Athena settings.
•	Athena automatically recognizes tables created by Glue Crawlers. For example, the table name matches the last folder name in S3 (e.g., enigma_jhud).
•	Use the Athena Query Editor to run SQL queries on your data.
•	Preview tables to inspect columns and sample data (e.g., use SELECT * FROM enigma_jhud LIMIT 10).
•	Athena supports partitioning based on S3 folder structure, which helps optimize queries when data is organized in subfolders like CSV1, CSV2, etc.
•	While you can use Python (with Pandas or other libraries) to read S3 data, Athena is more efficient and scalable for large datasets because it queries directly on S3 without data movement.
Why It Matters:
•	Athena enables serverless, scalable SQL querying on your data without ETL or data movement.
•	Using Glue catalog tables with Athena provides a seamless way to query semi-structured data stored on S3.
•	Partitioning improves query performance and reduces cost by limiting data scanned.
•	Athena is ideal for large datasets where local tools like Python would be inefficient.


 

 

